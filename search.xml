<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[测试评论]]></title>
    <url>%2Fposts%2F59148%2F</url>
    <content type="text"><![CDATA[测试]]></content>
  </entry>
  <entry>
    <title><![CDATA[多线程爬取图片]]></title>
    <url>%2Fposts%2F48904%2F</url>
    <content type="text"><![CDATA[在参考了很多网上很多大牛的技术，也想着自己写一个博客。之前写了一个爬虫工具，就拿这个来写吧。 首先感谢 崔庆才 http://cuiqingcai.com/3179.html 文章写的都挺好的。环境Python3.7 第三方库requestsBeautifulSoup 安装方法：pip install requestspip install beautifulsoup4 原理找到目标网址，根据获取的url进行解析遍历，在生成本地的html文本。从文本中获取你想要的资源，通过python来实现下载。 目标网址（一个很羞涩的网址） http://mzitu.com 步骤 首先获取网址 Hostreferer = { &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;, &#39;Referer&#39;: &#39;http://www.mzitu.com&#39; } start_html = requests.get(&#39;http://www.mzitu.com&#39;, headers=Hostreferer) soup = BeautifulSoup(start_html.text, &quot;html.parser&quot;) print(soup) ` 可以查看采集到的页面信息。 但是这个页面有很多页，并且每一页有单独的分类图片，随机选几页可以看到url都是一样的http://www.mzitu.com/page/num，后面是页数。 start_html = requests.get(&#39;http://www.mzitu.com/page/1&#39;, headers=Hostreferer) soup = BeautifulSoup(start_html.text, &quot;html.parser&quot;) all_a = soup.find(&#39;div&#39;, class_=&#39;postlist&#39;).find_all( &#39;a&#39;, target=&#39;_blank&#39;) for a in all_a: print(a) 可以看下爬取到这一页中每个分类的连接 现在开始爬取每个分类中的图片,从上个获取到的a标签中获取需要的url print(&quot;准备扒取：&quot; + title) html = requests.get(a[&#39;href&#39;], headers=Hostreferer) mess = BeautifulSoup(html.text, &quot;html.parser&quot;) pic_max = mess.find_all(&#39;span&#39;) pic_max = pic_max[10].text # 最大页数 for num in range(1, int(pic_max) + 1): pic = href + &#39;/&#39; + str(num) html = requests.get(pic, headers=Hostreferer) mess = BeautifulSoup(html.text, &quot;html.parser&quot;) pic_url = mess.find(&#39;img&#39;, alt=a.get_text()) print(pic_url[&#39;src&#39;]) html = requests.get(pic_url[&#39;src&#39;], headers=Picreferer) file_name = pic_url[&#39;src&#39;].split(r&#39;/&#39;)[-1] f = open(file_name, &#39;wb&#39;) f.write(html.content) f.close() print(&#39;完成&#39;) 我们把图片按照每个分类的名称存储，import os path = &#39;F:\\meitu\&#39; if(os.path.exists(path + title.strip().replace(&#39;?&#39;, &#39;&#39;))): # print(&#39;目录已存在&#39;) flag = 1 else: os.makedirs(path + title.strip().replace(&#39;?&#39;, &#39;&#39;)) flag = 0 os.chdir(path + title.strip().replace(&#39;?&#39;, &#39;&#39;)) if(flag == 1 and len(os.listdir(path + title.strip().replace(&#39;?&#39;, &#39;&#39;))) &gt;= int(pic_max)): print(&#39;已经保存完毕，跳过&#39;) return 1 多线程操作引入from multiprocessing import Pool将下载的方法放在多线程中操作 pool = Pool(processes=20) pool.apply_async(DownLoad, args=( href, title, path, Hostreferer, Picreferer)) 完整代码import requests from bs4 import BeautifulSoup import os from Thread_demo import * from multiprocessing import Pool def max_page(all_url, Hostreferer): max_page = 0 start_html = requests.get(all_url, headers=Hostreferer) soup = BeautifulSoup(start_html.text, &quot;html.parser&quot;) page = soup.find_all(&#39;a&#39;, class_=&#39;page-numbers&#39;) max_page = page[-2].text return max_page def Down(url): path = &#39;E:/meitu/&#39; Hostreferer = { &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;, &#39;Referer&#39;: &#39;http://www.mzitu.com&#39; } Picreferer = { &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;, &#39;Referer&#39;: &#39;http://i.meizitu.net&#39; } def DownLoad(href, title, path, Hostreferer, Picreferer): print(&quot;准备扒取：&quot; + title) # win不能创建带？的目录 if(os.path.exists(path + title.strip().replace(&#39;?&#39;, &#39;&#39;))): # print(&#39;目录已存在&#39;) flag = 1 else: os.makedirs(path + title.strip().replace(&#39;?&#39;, &#39;&#39;)) flag = 0 os.chdir(path + title.strip().replace(&#39;?&#39;, &#39;&#39;)) html = requests.get(href, headers=Hostreferer) mess = BeautifulSoup(html.text, &quot;html.parser&quot;) pic_max = mess.find_all(&#39;span&#39;) pic_max = pic_max[10].text # 最大页数 if(flag == 1 and len(os.listdir(path + title.strip().replace(&#39;?&#39;, &#39;&#39;))) &gt;= int(pic_max)): print(&#39;已经保存完毕，跳过&#39;) return 1 for num in range(1, int(pic_max) + 1): pic = href + &#39;/&#39; + str(num) html = requests.get(pic, headers=Hostreferer) mess = BeautifulSoup(html.text, &quot;html.parser&quot;) pic_url = mess.find(&#39;img&#39;, alt=title) print(pic_url[&#39;src&#39;]) # exit(0) html = requests.get(pic_url[&#39;src&#39;], headers=Picreferer) file_name = pic_url[&#39;src&#39;].split(r&#39;/&#39;)[-1] f = open(file_name, &#39;wb&#39;) f.write(html.content) f.close() print(&#39;完成&#39;) if __name__ == &#39;__main__&#39;: if (os.name == &#39;nt&#39;): print(u&#39;你正在使用win平台&#39;) else: print(u&#39;你正在使用linux平台&#39;) Hostreferer = { &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;, &#39;Referer&#39;: &#39;http://www.mzitu.com&#39; } Picreferer = { &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;, &#39;Referer&#39;: &#39;http://i.meizitu.net&#39; } all_url = &#39;http://www.mzitu.com&#39; same_url = &#39;http://www.mzitu.com/page/&#39; path = &#39;E:/meitu/&#39; max_page = max_page(all_url, Hostreferer) # 线程池中线程数 pool = Pool(processes=20) for n in range(1, int(max_page) + 1): ul = same_url + str(n) start_html = requests.get(ul, headers=Hostreferer) soup = BeautifulSoup(start_html.text, &quot;html.parser&quot;) all_a = soup.find(&#39;div&#39;, class_=&#39;postlist&#39;).find_all( &#39;a&#39;, target=&#39;_blank&#39;) for a in all_a: print(a) title = a.get_text() if (title != &#39;&#39;): href = a[&#39;href&#39;] pool.apply_async(DownLoad, args=( href, title, path, Hostreferer, Picreferer)) pool.close() pool.join() print(&#39;所有图片完成&#39;)]]></content>
  </entry>
  <entry>
    <title><![CDATA[互联网经济]]></title>
    <url>%2Fposts%2F24187%2F</url>
    <content type="text"><![CDATA[互联网经济 互联网经济是基于互联网所产生的经济活动的总和，在当今发展阶段主要 包括电子商务、互联网金融(ITFIN)、即时通讯、搜索引擎和网络游戏五大类型。互联网经济是信息网络化时代产生的一种崭新的经济现象。 在互联网经济时代，经济主体的生产、交换、分配、消费等经济活动，以及金融机构和政府职能部门等主体的经济行为，都越来越多地依赖信息网络，不仅要从网络上获取大量经济信息，依靠网络进行预测和决策，而且许多交易行为也直接在信息网络上 进行。 阿里巴巴和淘宝 总部位于杭州的阿里巴巴集团，其旗下的阿里巴巴B2B网站是全球最大的网上贸易市场。良好的定位，稳固的结构，优秀的服务使阿里巴巴B2B网站成为全球首家拥有210万商人的电子商务网站，成为全球商人网络推广的首选网站，被商人们评为”最受欢迎的B2B网站”。淘宝网是阿里巴巴集团旗下主营B2C、C2C业务的专业网站。淘宝网自2003年5月推出以来，短短五年内迅速发展成为亚洲最大的网络零售商圈。 截至2008年第一季度，淘宝网已为社会提供超过30万的直接就业岗位，并创造了60万的间接就业岗位。在不久前的淘宝网五周年庆典上，阿里巴巴集团董事局主席马云表示，阿里巴巴集团将向淘宝网追加20亿元人民币投资，使淘宝网5年内超越eBay全球和亚马逊，10年内超越沃尔玛全球，成为全球零售业老大。 ###网络经济(Network_Economy_)的介绍?### 网络经济(Network Economy ):国际互联网引发的经济革命 众所周知,知识经济是以电脑、卫星通信、光缆通信和数码技术等为标志的现代信息技术和全球信息网络”爆炸性”发展的必然结果。在知识经济条件下,现实经济运行主要表现为信息化和全球化两大趋势。这两种趋势的出现无不与信息技术和信息网络的发展密切相关。 现代信息技术的发展,大大提高了人们处理信息的能力和利用信息的效率,加速了科技开发与创新的步伐,加快了科技成果向现实生产力转化的速度,从而使知识在经济增长中的贡献程度空前提高; 全球信息网络的出现和发展,进一步加快了信息在全球范围内的传递和扩散,使传统的国家、民族界限变得日益模糊,使整个世界变成了一个小小的”地球村”,从而使世界经济发展呈现出明显的全球化趋势。 因此,知识经济实质上是一种以现代信息技术为核心的全球网络经济。 出处]]></content>
      <categories>
        <category>categories</category>
      </categories>
      <tags>
        <tag>tags</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[love]]></title>
    <url>%2Fposts%2F1739%2F</url>
    <content type="text"><![CDATA[I love you ，zyy –ydc]]></content>
      <categories>
        <category>爱情</category>
      </categories>
      <tags>
        <tag>情感</tag>
        <tag>恋爱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fposts%2F16107%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new "My New Post" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
